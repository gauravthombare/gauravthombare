{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOdUJKzuFkLUbfctLmivEIu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gauravthombare/gauravthombare/blob/main/tokenize_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Standard way"
      ],
      "metadata": {
        "id": "4vFx2KHgldSE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kr3uRk3LlUDy",
        "outputId": "28345f87-b3b9-440b-a470-0a5f19f815d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['There', 'are', 'multiple', 'ways', 'we', 'can', 'perform', 'tokenization', 'on', 'given', 'text', 'data.', 'We', 'can', 'choose', 'any', 'method', 'based', 'on', 'langauge,', 'library', 'and', 'purpose', 'of', 'modeling.']\n"
          ]
        }
      ],
      "source": [
        "text = \"\"\"There are multiple ways we can perform tokenization on given text data. We can choose any method based on langauge, library and purpose of modeling.\"\"\"\n",
        "\n",
        "tokens = text.split()\n",
        "\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Python split method do not consider punctuation as separate token.\n",
        "[reference](https://https://www.kaggle.com/code/satishgunjal/tokenization-in-nlp)"
      ],
      "metadata": {
        "id": "HQTiqu75lzbC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using Regex"
      ],
      "metadata": {
        "id": "ilnxqTvqmxNT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "text = \"\"\"There are multiple ways we can perform tokenization on given text data. We can choose any method based on langauge, library and purpose of modeling.\"\"\"\n",
        "tokens = re.findall(\"[\\w]+\", text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "4PcWeucbly1i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d77dd5a6-ea33-4ba9-da72-ffc0e3e8ab2a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['There', 'are', 'multiple', 'ways', 'we', 'can', 'perform', 'tokenization', 'on', 'given', 'text', 'data', 'We', 'can', 'choose', 'any', 'method', 'based', 'on', 'langauge', 'library', 'and', 'purpose', 'of', 'modeling']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[] :    A set of characters.\n",
        "\\w :    Returns a match where the string contains any word characters (characters from a to Z, digits from 0-9, and the underscore _ character).\n",
        "\\+  :    One or more occurrences."
      ],
      "metadata": {
        "id": "BdbKUBBdm_Ia"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization using NLTK\n",
        "Natual language tool kit\n",
        "\n",
        "module\n",
        "word_tokenize() - word tokenization\n",
        "sent_tokenize() - sentence tokenization"
      ],
      "metadata": {
        "id": "O1I-CFCEnJQK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "zqn06GzxnA5z"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rsdb9fIWntsC",
        "outputId": "ff5af263-a944-4f25-b789-8c29c1bef7c6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"There are multiple ways we can perform tokenization on given text data. We can choose any method based on langauge, library and purpose of modeling.\"\"\"\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ku0NkOFMnmuL",
        "outputId": "af95d40c-7783-48dd-88b3-a3925c185b99"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['There', 'are', 'multiple', 'ways', 'we', 'can', 'perform', 'tokenization', 'on', 'given', 'text', 'data', '.', 'We', 'can', 'choose', 'any', 'method', 'based', 'on', 'langauge', ',', 'library', 'and', 'purpose', 'of', 'modeling', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization using spaCy\n",
        "\n",
        "open source library for advanced natual language processing written in python and cython"
      ],
      "metadata": {
        "id": "a1qjgVP9n8Xb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.lang.en import English"
      ],
      "metadata": {
        "id": "LBEtOOg5oO6C"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load English tokenizer. \n",
        "# nlp object will be used to create 'doc' object which uses preprecoessing pipeline's components such as tagger, parser, NER and word vectors\n",
        "nlp = English()\n",
        "\n",
        "text = \"\"\"There are multiple ways we can perform tokenization on given text data. We can choose any method based on langauge, library and purpose of modeling.\"\"\"\n",
        "\n",
        "# Now we will process above text using 'nlp' object. Which is use to create documents with linguistic annotations and various nlp properties\n",
        "my_doc = nlp(text)\n",
        "\n",
        "# Above step has already tokenized our text but its in doc format, so lets write fo loop to create list of it\n",
        "token_list = []\n",
        "for token in my_doc:\n",
        "    token_list.append(token.text)\n",
        "\n",
        "print(token_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSzKTqr2oYhB",
        "outputId": "ec39b5d1-40c3-458f-8e4b-ef6d2019b278"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['There', 'are', 'multiple', 'ways', 'we', 'can', 'perform', 'tokenization', 'on', 'given', 'text', 'data', '.', 'We', 'can', 'choose', 'any', 'method', 'based', 'on', 'langauge', ',', 'library', 'and', 'purpose', 'of', 'modeling', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization using Keras\n",
        "Keras is opensource neural network library written in python. It is easy to use and it is capable of running on top of TensorFlow, Microsoft Cognitive Toolkit, R, Theano, or PlaidML\n",
        "To perform word tokenization we use the text_to_word_sequence() method from the keras.preprocessing.text class\n",
        "By default, this function automatically does 3 things:\n",
        "Splits words by space (split=” “).\n",
        "Filters out punctuation (filters=’!”#$%&()*+,-./:;<=>?@[\\]^_`{|}~\\t\\n’).\n",
        "Converts text to lowercase (lower=True)."
      ],
      "metadata": {
        "id": "WO5KCoYlotyh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "\n",
        "text = \"\"\"There are multiple ways we can perform tokenization on given text data. We can choose any method based on langauge, library and purpose of modeling.\"\"\"\n",
        "\n",
        "tokens = text_to_word_sequence(text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvxlM4Sho2vq",
        "outputId": "eea9bb57-6e23-49e4-8041-3b6fa90e867b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['there', 'are', 'multiple', 'ways', 'we', 'can', 'perform', 'tokenization', 'on', 'given', 'text', 'data', 'we', 'can', 'choose', 'any', 'method', 'based', 'on', 'langauge', 'library', 'and', 'purpose', 'of', 'modeling']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization using Gensim\n",
        "Gensim is an open-source library for unsupervised topic modeling and natural language processing, using modern statistical machine learning.\n",
        "We are going to use tokenize() from gensim.utility class for word tokenization.\n",
        "Unlike other libraries Gensim has separate method split_sentences() from class gensim.summarization.textcleaner for sentence tokenization."
      ],
      "metadata": {
        "id": "6eO6S8dSpMWC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.utils import tokenize\n",
        "\n",
        "text = \"\"\"There are multiple ways we can perform tokenization on given text data. We can choose any method based on langauge, library and purpose of modeling.\"\"\"\n",
        "\n",
        "tokens = list(tokenize(text))\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4M7rjqeCpaGK",
        "outputId": "803886c7-dcd3-4d41-efef-c997dc992653"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['There', 'are', 'multiple', 'ways', 'we', 'can', 'perform', 'tokenization', 'on', 'given', 'text', 'data', 'We', 'can', 'choose', 'any', 'method', 'based', 'on', 'langauge', 'library', 'and', 'purpose', 'of', 'modeling']\n"
          ]
        }
      ]
    }
  ]
}
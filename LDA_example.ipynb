{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pyldavis\n",
      "  Using cached pyLDAvis-3.3.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: sklearn in c:\\users\\gt\\appdata\\roaming\\python\\python38\\site-packages (from pyldavis) (0.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\gt\\appdata\\roaming\\python\\python38\\site-packages (from pyldavis) (1.0.2)\n",
      "Collecting gensim\n",
      "  Using cached gensim-4.2.0-cp38-cp38-win_amd64.whl (24.0 MB)\n",
      "Requirement already satisfied: pandas>=1.2.0 in c:\\users\\gt\\appdata\\roaming\\python\\python38\\site-packages (from pyldavis) (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.20.0 in c:\\users\\gt\\appdata\\roaming\\python\\python38\\site-packages (from pyldavis) (1.22.4)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  DEPRECATION: future is being installed using the legacy 'setup.py install' method, because it does not have a 'pyproject.toml' and the 'wheel' package is not installed. pip 23.1 will enforce this behaviour change. A possible replacement is to enable the '--use-pep517' option. Discussion can be found at https://github.com/pypa/pip/issues/8559\n",
      "  WARNING: The scripts cygdb.exe, cython.exe and cythonize.exe are installed in 'C:\\Users\\GT\\AppData\\Roaming\\Python\\Python38\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: jinja2 in c:\\users\\gt\\appdata\\roaming\\python\\python38\\site-packages (from pyldavis) (2.11.3)\n",
      "Collecting numexpr\n",
      "  Using cached numexpr-2.8.4-cp38-cp38-win_amd64.whl (92 kB)\n",
      "Requirement already satisfied: scipy in c:\\users\\gt\\appdata\\roaming\\python\\python38\\site-packages (from pyldavis) (1.6.3)\n",
      "Requirement already satisfied: setuptools in c:\\program files\\python38\\lib\\site-packages (from pyldavis) (49.2.1)\n",
      "Collecting future\n",
      "  Using cached future-0.18.2.tar.gz (829 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: funcy in c:\\users\\gt\\appdata\\roaming\\python\\python38\\site-packages (from pyldavis) (1.17)\n",
      "Requirement already satisfied: joblib in c:\\users\\gt\\appdata\\roaming\\python\\python38\\site-packages (from pyldavis) (1.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\gt\\appdata\\roaming\\python\\python38\\site-packages (from pandas>=1.2.0->pyldavis) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\gt\\appdata\\roaming\\python\\python38\\site-packages (from pandas>=1.2.0->pyldavis) (2021.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\gt\\appdata\\roaming\\python\\python38\\site-packages (from gensim->pyldavis) (6.3.0)\n",
      "Collecting Cython==0.29.28\n",
      "  Using cached Cython-0.29.28-py2.py3-none-any.whl (983 kB)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\gt\\appdata\\roaming\\python\\python38\\site-packages (from jinja2->pyldavis) (1.1.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\gt\\appdata\\roaming\\python\\python38\\site-packages (from scikit-learn->pyldavis) (3.1.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\gt\\appdata\\roaming\\python\\python38\\site-packages (from python-dateutil>=2.7.3->pandas>=1.2.0->pyldavis) (1.16.0)\n",
      "Installing collected packages: numexpr, future, Cython, gensim, pyldavis\n",
      "  Running setup.py install for future: started\n",
      "  Running setup.py install for future: finished with status 'done'\n",
      "Successfully installed Cython-0.29.28 future-0.18.2 gensim-4.2.0 numexpr-2.8.4 pyldavis-3.3.1\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gensim in c:\\users\\gt\\appdata\\roaming\\python\\python38\\site-packages (4.2.0)\n",
      "Requirement already satisfied: pyLDAvis in c:\\users\\gt\\appdata\\roaming\\python\\python38\\site-packages (3.3.1)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\gt\\appdata\\roaming\\python\\python38\\site-packages (from gensim) (1.22.4)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\gt\\appdata\\roaming\\python\\python38\\site-packages (from gensim) (1.6.3)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\gt\\appdata\\roaming\\python\\python38\\site-packages (from gensim) (6.3.0)\n",
      "Requirement already satisfied: Cython==0.29.28 in c:\\users\\gt\\appdata\\roaming\\python\\python38\\site-packages (from gensim) (0.29.28)\n",
      "Requirement already satisfied: funcy in c:\\users\\gt\\appdata\\roaming\\python\\python38\\site-packages (from pyLDAvis) (1.17)\n",
      "Requirement already satisfied: future in c:\\users\\gt\\appdata\\roaming\\python\\python38\\site-packages (from pyLDAvis) (0.18.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\gt\\appdata\\roaming\\python\\python38\\site-packages (from pyLDAvis) (1.1.0)\n",
      "Requirement already satisfied: setuptools in c:\\program files\\python38\\lib\\site-packages (from pyLDAvis) (49.2.1)\n",
      "Requirement already satisfied: pandas>=1.2.0 in c:\\users\\gt\\appdata\\roaming\\python\\python38\\site-packages (from pyLDAvis) (1.2.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\gt\\appdata\\roaming\\python\\python38\\site-packages (from pyLDAvis) (2.11.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\gt\\appdata\\roaming\\python\\python38\\site-packages (from pyLDAvis) (1.0.2)\n",
      "Requirement already satisfied: numexpr in c:\\users\\gt\\appdata\\roaming\\python\\python38\\site-packages (from pyLDAvis) (2.8.4)\n",
      "Requirement already satisfied: sklearn in c:\\users\\gt\\appdata\\roaming\\python\\python38\\site-packages (from pyLDAvis) (0.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\gt\\appdata\\roaming\\python\\python38\\site-packages (from pandas>=1.2.0->pyLDAvis) (2021.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\gt\\appdata\\roaming\\python\\python38\\site-packages (from pandas>=1.2.0->pyLDAvis) (2.8.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\gt\\appdata\\roaming\\python\\python38\\site-packages (from jinja2->pyLDAvis) (1.1.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\gt\\appdata\\roaming\\python\\python38\\site-packages (from scikit-learn->pyLDAvis) (3.1.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\gt\\appdata\\roaming\\python\\python38\\site-packages (from python-dateutil>=2.7.3->pandas>=1.2.0->pyLDAvis) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python was not found; run without arguments to install from the Microsoft Store, or disable this shortcut from Settings > Manage App Execution Aliases.\n"
     ]
    }
   ],
   "source": [
    "!pip install pyldavis\n",
    "!pip install gensim pyLDAvis\n",
    "! python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.7-py3-none-any.whl (1.5 MB)\n",
      "     ---------------------------------------- 1.5/1.5 MB 1.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: joblib in c:\\users\\gt\\appdata\\roaming\\python\\python38\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: click in c:\\users\\gt\\appdata\\roaming\\python\\python38\\site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\gt\\appdata\\roaming\\python\\python38\\site-packages (from nltk) (4.64.1)\n",
      "Collecting regex>=2021.8.3\n",
      "  Downloading regex-2022.10.31-cp38-cp38-win_amd64.whl (267 kB)\n",
      "     -------------------------------------- 267.7/267.7 kB 2.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: colorama in c:\\users\\gt\\appdata\\roaming\\python\\python38\\site-packages (from click->nltk) (0.4.4)\n",
      "Installing collected packages: regex, nltk\n",
      "Successfully installed nltk-3.7 regex-2022.10.31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script nltk.exe is installed in 'C:\\Users\\GT\\AppData\\Roaming\\Python\\Python38\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting spacy\n",
      "  Downloading spacy-3.4.3-cp38-cp38-win_amd64.whl (12.2 MB)\n",
      "     ---------------------------------------- 12.2/12.2 MB 2.0 MB/s eta 0:00:00\n",
      "Collecting pathy>=0.3.5\n",
      "  Downloading pathy-0.10.1-py3-none-any.whl (48 kB)\n",
      "     ---------------------------------------- 48.9/48.9 kB 2.4 MB/s eta 0:00:00\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.10\n",
      "  Downloading spacy_legacy-3.0.10-py2.py3-none-any.whl (21 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.9-cp38-cp38-win_amd64.whl (18 kB)\n",
      "Collecting typer<0.8.0,>=0.3.0\n",
      "  Downloading typer-0.7.0-py3-none-any.whl (38 kB)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "     -------------------------------------- 181.6/181.6 kB 2.2 MB/s eta 0:00:00\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Downloading spacy_loggers-1.0.4-py3-none-any.whl (11 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.8-py3-none-any.whl (17 kB)\n",
      "Collecting thinc<8.2.0,>=8.1.0\n",
      "  Downloading thinc-8.1.5-cp38-cp38-win_amd64.whl (1.3 MB)\n",
      "     ---------------------------------------- 1.3/1.3 MB 2.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\gt\\appdata\\roaming\\python\\python38\\site-packages (from spacy) (2.25.1)\n",
      "Requirement already satisfied: setuptools in c:\\program files\\python38\\lib\\site-packages (from spacy) (49.2.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\gt\\appdata\\roaming\\python\\python38\\site-packages (from spacy) (1.22.4)\n",
      "Collecting tqdm<5.0.0,>=4.38.0\n",
      "  Downloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
      "     ---------------------------------------- 78.5/78.5 kB 2.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\gt\\appdata\\roaming\\python\\python38\\site-packages (from spacy) (21.3)\n",
      "Collecting wasabi<1.1.0,>=0.9.1\n",
      "  Downloading wasabi-0.10.1-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\gt\\appdata\\roaming\\python\\python38\\site-packages (from spacy) (2.11.3)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.8-cp38-cp38-win_amd64.whl (96 kB)\n",
      "     ---------------------------------------- 96.7/96.7 kB 1.4 MB/s eta 0:00:00\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4\n",
      "  Downloading pydantic-1.10.2-cp38-cp38-win_amd64.whl (2.2 MB)\n",
      "     ---------------------------------------- 2.2/2.2 MB 2.0 MB/s eta 0:00:00\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Downloading srsly-2.4.5-cp38-cp38-win_amd64.whl (481 kB)\n",
      "     -------------------------------------- 481.3/481.3 kB 2.7 MB/s eta 0:00:00\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.7-cp38-cp38-win_amd64.whl (30 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\gt\\appdata\\roaming\\python\\python38\\site-packages (from packaging>=20.0->spacy) (2.4.7)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\gt\\appdata\\roaming\\python\\python38\\site-packages (from pathy>=0.3.5->spacy) (6.3.0)\n",
      "Collecting typing-extensions>=4.1.0\n",
      "  Downloading typing_extensions-4.4.0-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gt\\appdata\\roaming\\python\\python38\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.12.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\gt\\appdata\\roaming\\python\\python38\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\gt\\appdata\\roaming\\python\\python38\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\gt\\appdata\\roaming\\python\\python38\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Collecting blis<0.8.0,>=0.7.8\n",
      "  Downloading blis-0.7.9-cp38-cp38-win_amd64.whl (7.0 MB)\n",
      "     ---------------------------------------- 7.0/7.0 MB 2.1 MB/s eta 0:00:00\n",
      "Collecting confection<1.0.0,>=0.0.1\n",
      "  Downloading confection-0.0.3-py3-none-any.whl (32 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\gt\\appdata\\roaming\\python\\python38\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.4)\n",
      "Collecting click<9.0.0,>=7.1.1\n",
      "  Downloading click-8.1.3-py3-none-any.whl (96 kB)\n",
      "     ---------------------------------------- 96.6/96.6 kB 1.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\gt\\appdata\\roaming\\python\\python38\\site-packages (from jinja2->spacy) (1.1.1)\n",
      "Installing collected packages: wasabi, cymem, typing-extensions, tqdm, spacy-loggers, spacy-legacy, murmurhash, langcodes, click, catalogue, blis, typer, srsly, pydantic, preshed, pathy, confection, thinc, spacy\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.0.1\n",
      "    Uninstalling typing_extensions-4.0.1:\n",
      "      Successfully uninstalled typing_extensions-4.0.1\n",
      "Successfully installed blis-0.7.9 catalogue-2.0.8 click-8.1.3 confection-0.0.3 cymem-2.0.7 langcodes-3.3.0 murmurhash-1.0.9 pathy-0.10.1 preshed-3.0.8 pydantic-1.10.2 spacy-3.4.3 spacy-legacy-3.0.10 spacy-loggers-1.0.4 srsly-2.4.5 thinc-8.1.5 tqdm-4.64.1 typer-0.7.0 typing-extensions-4.4.0 wasabi-0.10.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script tqdm.exe is installed in 'C:\\Users\\GT\\AppData\\Roaming\\Python\\Python38\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script pathy.exe is installed in 'C:\\Users\\GT\\AppData\\Roaming\\Python\\Python38\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script spacy.exe is installed in 'C:\\Users\\GT\\AppData\\Roaming\\Python\\Python38\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\GT\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\GT\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ” Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_md')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import string\n",
    "import spacy\n",
    "import nltk\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet as wn\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import spacy.cli\n",
    "spacy.cli.download('en_core_web_md')\n",
    "import en_core_web_md\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = r\"C:\\Users\\GT\\Documents\\python\\read_docx_file\"\n",
    "file_name = 'yelp.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>date</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>user_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9yKzy9PApeiPPOUJEtnvkg</td>\n",
       "      <td>2011-01-26</td>\n",
       "      <td>fWKvX83p0-ka4JS3dc6E5A</td>\n",
       "      <td>5</td>\n",
       "      <td>My wife took me here on my birthday for breakf...</td>\n",
       "      <td>review</td>\n",
       "      <td>rLtl8ZkDX5vH5nAx9C3q5Q</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ZRJwVLyzEJq1VAihDhYiow</td>\n",
       "      <td>2011-07-27</td>\n",
       "      <td>IjZ33sJrzXqU-0X6U8NwyA</td>\n",
       "      <td>5</td>\n",
       "      <td>I have no idea why some people give bad review...</td>\n",
       "      <td>review</td>\n",
       "      <td>0a2KyEL0d3Yb1V6aivbIuQ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6oRAC4uyJCsJl1X0WZpVSA</td>\n",
       "      <td>2012-06-14</td>\n",
       "      <td>IESLBzqUCLdSzSqm0eCSxQ</td>\n",
       "      <td>4</td>\n",
       "      <td>love the gyro plate. Rice is so good and I als...</td>\n",
       "      <td>review</td>\n",
       "      <td>0hT2KtfLiobPvh6cDC8JQg</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>_1QQZuf4zZOyFCvXc0o6Vg</td>\n",
       "      <td>2010-05-27</td>\n",
       "      <td>G-WvGaISbqqaMHlNnByodA</td>\n",
       "      <td>5</td>\n",
       "      <td>Rosie, Dakota, and I LOVE Chaparral Dog Park!!...</td>\n",
       "      <td>review</td>\n",
       "      <td>uZetl9T0NcROGOyFfughhg</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6ozycU1RpktNG2-1BroVtw</td>\n",
       "      <td>2012-01-05</td>\n",
       "      <td>1uJFq2r5QfJG_6ExMRCaGw</td>\n",
       "      <td>5</td>\n",
       "      <td>General Manager Scott Petello is a good egg!!!...</td>\n",
       "      <td>review</td>\n",
       "      <td>vYmM4KTsC8ZfQBg-j5MWkw</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id        date               review_id  stars  \\\n",
       "0  9yKzy9PApeiPPOUJEtnvkg  2011-01-26  fWKvX83p0-ka4JS3dc6E5A      5   \n",
       "1  ZRJwVLyzEJq1VAihDhYiow  2011-07-27  IjZ33sJrzXqU-0X6U8NwyA      5   \n",
       "2  6oRAC4uyJCsJl1X0WZpVSA  2012-06-14  IESLBzqUCLdSzSqm0eCSxQ      4   \n",
       "3  _1QQZuf4zZOyFCvXc0o6Vg  2010-05-27  G-WvGaISbqqaMHlNnByodA      5   \n",
       "4  6ozycU1RpktNG2-1BroVtw  2012-01-05  1uJFq2r5QfJG_6ExMRCaGw      5   \n",
       "\n",
       "                                                text    type  \\\n",
       "0  My wife took me here on my birthday for breakf...  review   \n",
       "1  I have no idea why some people give bad review...  review   \n",
       "2  love the gyro plate. Rice is so good and I als...  review   \n",
       "3  Rosie, Dakota, and I LOVE Chaparral Dog Park!!...  review   \n",
       "4  General Manager Scott Petello is a good egg!!!...  review   \n",
       "\n",
       "                  user_id  cool  useful  funny  \n",
       "0  rLtl8ZkDX5vH5nAx9C3q5Q     2       5      0  \n",
       "1  0a2KyEL0d3Yb1V6aivbIuQ     0       0      0  \n",
       "2  0hT2KtfLiobPvh6cDC8JQg     0       1      0  \n",
       "3  uZetl9T0NcROGOyFfughhg     1       2      0  \n",
       "4  vYmM4KTsC8ZfQBg-j5MWkw     0       0      0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp_review = pd.read_csv(file_name)\n",
    "yelp_review.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Reviews\n",
      "10000\n",
      "Unique Business\n",
      "4174\n",
      "Unique User\n",
      "6403\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Reviews\")\n",
    "print(len(yelp_review))\n",
    "print(\"Unique Business\")\n",
    "print(len(yelp_review.groupby('business_id')))\n",
    "print(\"Unique User\")\n",
    "print(len(yelp_review.groupby('user_id')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the document and remove punctuation\n",
    "def clean_text(text):\n",
    "    delete_dict = {sp_char: '' for sp_char in string.punctuation}\n",
    "    delete_dict[' '] =' '\n",
    "    table = str.maketrans(delete_dict)\n",
    "    text1 = text.translate(table)\n",
    "    textArr= text1.split()\n",
    "    text2 = ' '.join([w for w in textArr if (not w.isdigit() and ( not w.isdigit() and len(w)>3))])\n",
    "    return text2.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_review['text_new'] = yelp_review['text'].apply(clean_text)\n",
    "yelp_review['Num_words_text'] = yelp_review['text_new'].apply(lambda x:len(str(x).split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------Reviews By Stars --------\n",
      "4    3526\n",
      "5    3337\n",
      "3    1461\n",
      "2     927\n",
      "1     749\n",
      "Name: stars, dtype: int64\n",
      "10000\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "print('-------Reviews By Stars --------')\n",
    "print(yelp_review['stars'].value_counts())\n",
    "print(len(yelp_review))\n",
    "print('-------------------------')\n",
    "max_review_data_sentence_length  = yelp_review['Num_words_text'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of Short reviews\n",
      "6276\n"
     ]
    }
   ],
   "source": [
    " # print short review (\n",
    "mask = (yelp_review['Num_words_text'] < 100) & (yelp_review['Num_words_text'] >=20)\n",
    "df_short_reviews = yelp_review[mask]\n",
    "df_sampled = df_short_reviews.groupby('stars').apply(lambda x: x.sample(n=100)).reset_index(drop = True)\n",
    " \n",
    "print('No of Short reviews')\n",
    "print(len(df_short_reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to remove stopwords\n",
    "def remove_stopwords(text):\n",
    "    textArr = text.split(' ')\n",
    "    rem_text = \" \".join([i for i in textArr if i not in stop_words])\n",
    "    return rem_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords from the text\n",
    "stop_words = stopwords.words('english')\n",
    "df_sampled['text_new']=df_sampled['text_new'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform Lemmatization\n",
    "lp = en_core_web_md.load(disable=['parser', 'ner'])\n",
    "def lemmatization(texts,allowed_postags=['NOUN', 'ADJ']):\n",
    "    output = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(sent)\n",
    "        output.append([token.lemma_ for token in doc if token.pos_ in allowed_postags ])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list=df_sampled['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "May have great food but I'll never know. Ordered delivery and the chef who doesn't speak English couldn't find my apartment complex even when I walked out to the street right next to the 101 far away. He said he was there but couldn't tell me where he was. Talked to him on the phone for ten minutes then hung up on me when I guess he got frustrated and wouldn't answer my calls back. Called to order food for my visiting family and was glad to find this place but that chef is awful. The owner apologized to me profusely and that was a nice gesture but waiting an hour for a guy who hangs up on you and can't find a place looking at the 101 exit is ridiculous. Don't use them for delivery ever, call the Breakfast Joynt. Their owner is awesome and delivers to you himself. Plus, he actually knows the area. Cannot express my frustration with this restaurant's service. Do not use EVER\n",
      "['great', 'food', 'delivery', 'chef', 'apartment', 'complex', 'street', 'phone', 'minute', 'call', 'food', 'family', 'glad', 'place', 'chef', 'awful', 'owner', 'nice', 'gesture', 'hour', 'guy', 'place', 'exit', 'ridiculous', 'delivery', 'owner', 'awesome', 'area', 'frustration', 'restaurant', 'service']\n"
     ]
    }
   ],
   "source": [
    "print(text_list[2])\n",
    "tokenized_reviews = lemmatization(text_list)\n",
    "print(tokenized_reviews[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to document term frequency:\n",
    "dictionary = corpora.Dictionary(tokenized_reviews)\n",
    "doc_term_matrix = [dictionary.doc2bow(rev) for rev in tokenized_reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the object for LDA model using gensim library\n",
    "LDA = gensim.models.ldamodel.LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "lda_model = LDA(corpus=doc_term_matrix, id2word=dictionary,\n",
    "                num_topics=10, random_state=100,\n",
    "                chunksize=1000, passes=50,iterations=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.022*\"good\" + 0.020*\"food\" + 0.016*\"time\" + 0.014*\"place\" + 0.012*\"service\" + 0.010*\"great\" + 0.009*\"nice\" + 0.008*\"pizza\" + 0.006*\"drink\" + 0.006*\"lunch\"'),\n",
       " (1,\n",
       "  '0.015*\"time\" + 0.013*\"place\" + 0.013*\"great\" + 0.009*\"restaurant\" + 0.008*\"price\" + 0.007*\"dish\" + 0.007*\"several\" + 0.006*\"lot\" + 0.006*\"staff\" + 0.006*\"home\"'),\n",
       " (2,\n",
       "  '0.021*\"place\" + 0.018*\"food\" + 0.017*\"good\" + 0.010*\"bad\" + 0.010*\"restaurant\" + 0.008*\"staff\" + 0.008*\"time\" + 0.008*\"few\" + 0.008*\"friend\" + 0.008*\"more\"'),\n",
       " (3,\n",
       "  '0.017*\"place\" + 0.014*\"order\" + 0.010*\"good\" + 0.008*\"service\" + 0.007*\"well\" + 0.007*\"drink\" + 0.007*\"title\" + 0.007*\"little\" + 0.006*\"nice\" + 0.006*\"lot\"'),\n",
       " (4,\n",
       "  '0.014*\"food\" + 0.012*\"good\" + 0.012*\"restaurant\" + 0.011*\"service\" + 0.009*\"place\" + 0.009*\"lot\" + 0.009*\"buffet\" + 0.009*\"day\" + 0.008*\"lunch\" + 0.008*\"bit\"'),\n",
       " (5,\n",
       "  '0.023*\"place\" + 0.015*\"food\" + 0.012*\"time\" + 0.010*\"good\" + 0.010*\"service\" + 0.009*\"price\" + 0.009*\"location\" + 0.009*\"people\" + 0.009*\"sandwich\" + 0.008*\"happy\"'),\n",
       " (6,\n",
       "  '0.013*\"place\" + 0.010*\"food\" + 0.009*\"good\" + 0.009*\"great\" + 0.009*\"service\" + 0.007*\"thing\" + 0.007*\"time\" + 0.007*\"bus\" + 0.006*\"bar\" + 0.006*\"minute\"'),\n",
       " (7,\n",
       "  '0.042*\"good\" + 0.022*\"food\" + 0.020*\"great\" + 0.015*\"place\" + 0.012*\"service\" + 0.010*\"pizza\" + 0.009*\"time\" + 0.008*\"bar\" + 0.008*\"restaurant\" + 0.007*\"bad\"'),\n",
       " (8,\n",
       "  '0.023*\"place\" + 0.022*\"great\" + 0.019*\"food\" + 0.012*\"tea\" + 0.011*\"good\" + 0.009*\"burger\" + 0.007*\"time\" + 0.007*\"fry\" + 0.007*\"price\" + 0.006*\"other\"'),\n",
       " (9,\n",
       "  '0.023*\"place\" + 0.020*\"good\" + 0.019*\"time\" + 0.015*\"food\" + 0.010*\"great\" + 0.009*\"order\" + 0.008*\"friend\" + 0.008*\"thing\" + 0.008*\"lunch\" + 0.008*\"service\"')]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print lda topics with respect to each word of document\n",
    "lda_model.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity:  -6.720699287236979\n"
     ]
    }
   ],
   "source": [
    "# calculate perplexity and coherence\n",
    "print('Perplexity: ', lda_model.log_perplexity(doc_term_matrix,\n",
    "                                                total_docs=10000)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate coherence\n",
    "coherence_model_lda = CoherenceModel(model=lda_model,\n",
    "                                     texts=tokenized_reviews, dictionary=dictionary ,\n",
    "                                     coherence='c_v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence:  0.3128874006286301\n"
     ]
    }
   ],
   "source": [
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('Coherence: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(random_state=0)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_vectorizer = CountVectorizer(strip_accents = 'unicode',\n",
    "                                stop_words = 'english',\n",
    "                                lowercase = True,\n",
    "                                token_pattern = r'\\b[a-zA-Z]{3,}\\b',\n",
    "                                max_df = 0.5, \n",
    "                                min_df = 10)\n",
    "dtm_tf = tf_vectorizer.fit_transform(df_sampled['text_new'])\n",
    "\n",
    "# for TF DTM\n",
    "lda_tf = LatentDirichletAllocation(n_components=10, random_state=0)\n",
    "lda_tf.fit(dtm_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CountVectorizer' object has no attribute 'token2id'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1460/2475882068.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Now, we use pyLDA vis to visualize it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgensim_models\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlda_tf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtm_tf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf_vectorizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pyLDAvis\\gensim_models.py\u001b[0m in \u001b[0;36mprepare\u001b[1;34m(topic_model, corpus, dictionary, doc_topic_dist, **kwargs)\u001b[0m\n\u001b[0;32m    120\u001b[0m     \u001b[0mSee\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mpyLDAvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m     \"\"\"\n\u001b[1;32m--> 122\u001b[1;33m     \u001b[0mopts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_extract_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopic_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc_topic_dist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    123\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mopts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pyLDAvis\\gensim_models.py\u001b[0m in \u001b[0;36m_extract_data\u001b[1;34m(topic_model, corpus, dictionary, doc_topic_dists)\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mcorpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSparse2Corpus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus_csc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[0mvocab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoken2id\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m     \u001b[1;31m# TODO: add the hyperparam to smooth it out? no beta in online LDA impl.. hmm..\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;31m# for now, I'll just make sure we don't ever get zeros...\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'CountVectorizer' object has no attribute 'token2id'"
     ]
    }
   ],
   "source": [
    "# Now, we use pyLDA vis to visualize it\n",
    "pyLDAvis.gensim_models.prepare(lda_tf, dtm_tf, tf_vectorizer)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "570feb405e2e27c949193ac68f46852414290d515b0ba6e5d90d076ed2284471"
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
